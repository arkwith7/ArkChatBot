{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\phs\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Created on 2018. 2. 14.\n",
    "\n",
    "@author: phs\n",
    "\n",
    "1. 대화 말뭉치 파일을 읽어들인다.\n",
    "2. 딥러닝(tensorflow) 모델 생성에 사용한 자료구조를 읽어 들여서 반환한다.\n",
    "3. 딥러닝(tensorflow)으로 생성한 자연어 이해 모델을 읽어들여서 반환한다.\n",
    "4. 입력받은 문장을 자연어 처리한다(토크나이저, 스태밍).\n",
    "5. 자연어 처리한 문장을 Bag of word 생성하여 반환한다.\n",
    "6. 입력받은 문장을 학습 모델(자연어 이해 모델)을 이용 분류하여  결과를 반환한다.\n",
    "7. 분류된 말뭉치 대화에서 임의로 한 문장을 선택하여 입력받은 문장의 대답으로 반혼한다.\n",
    "\n",
    "'''\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\"\n",
    "\n",
    "# things we need for NLP\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# import nltk\n",
    "# from nltk.stem.lancaster import LancasterStemmer\n",
    "# stemmer = LancasterStemmer()\n",
    "\n",
    "# from konlpy.tag import Komoran\n",
    "from konlpy.tag import Twitter\n",
    "# komoran = Komoran()\n",
    "twitter = Twitter()\n",
    "\n",
    "import tflearn\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our chat-bot intents file\n",
    "def read_dialog_intents_jsonfile(input_file_name):\n",
    "    \"\"\"\n",
    "     대화 말뭉치 파일을 읽어들인다.\n",
    "    \"\"\"\n",
    "    with open(input_file_name, 'rt', encoding='UTF8') as json_data:\n",
    "        intents = json.load(json_data)\n",
    "        \n",
    "    return intents\n",
    "\n",
    "# 대화 말뭉치와 대화 의도가 정의된 JSON 문서 집합 읽기\n",
    "input_file_name = './DialogIntents/intents_home_kr.json'\n",
    "intents = read_dialog_intents_jsonfile(input_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore all of our data structures\n",
    "def restore_training_data_structures(input_training_data_file_name):\n",
    "    \"\"\"\n",
    "     딥러닝(tensorflow) 모델 생성에 사용한 자료구조를 읽어 들여서 반환한다.\n",
    "    \"\"\"\n",
    "    # restore all of our data structures\n",
    "    data = pickle.load( open( input_training_data_file_name, \"rb\" ) )\n",
    "    words = data['words']\n",
    "    classes = data['classes']\n",
    "    train_x = data['train_x']\n",
    "    train_y = data['train_y']\n",
    "    \n",
    "    return classes, words, train_x, train_y\n",
    "\n",
    "#  딥러닝(tensorflow) 모델 생성에 사용한 자료구조를 읽어들임\n",
    "input_training_data_file_name = \"./NLUModel/training_data_home_kr\"\n",
    "classes, words, train_x, train_y = restore_training_data_structures(input_training_data_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\phs\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tflearn\\objectives.py:66: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "# restore all of trained tflearn model file\n",
    "def restore_training_model(train_x, train_y, tflearn_logs_dir):\n",
    "    \"\"\"\n",
    "     딥러닝(tensorflow)으로 생성한 자연어 이해 모델을 읽어들여서 반환한다.\n",
    "    \"\"\"\n",
    "    # Build neural network\n",
    "    net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "    net = tflearn.fully_connected(net, 8)\n",
    "    net = tflearn.fully_connected(net, 8)\n",
    "    net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
    "    net = tflearn.regression(net)\n",
    "    \n",
    "    # Define model and setup tensorboard\n",
    "    model = tflearn.DNN(net, tensorboard_dir=tflearn_logs_dir)\n",
    "\n",
    "    return model\n",
    "\n",
    "#  딥러닝(tensorflow)으로 생성한 자연어 이해 모델을 읽어들임\n",
    "tflearn_logs_dir = \"home_tflearn_kr_logs\"\n",
    "model = restore_training_model(train_x, train_y, tflearn_logs_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\phs\\textmining\\python\\text-mining-camp\\note\\arkwith\\home_chat\\NLUModel\\model_home_kr.tflearn\n"
     ]
    }
   ],
   "source": [
    "# load our saved model\n",
    "tflearn_model_file_name = \"./NLUModel/model_home_kr.tflearn\"\n",
    "model.load(tflearn_model_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-25-a47e5141b81f>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-25-a47e5141b81f>\"\u001b[1;36m, line \u001b[1;32m15\u001b[0m\n\u001b[1;33m    print(\"sentence pos_result[][%s]\" % pos_result[])\u001b[0m\n\u001b[1;37m                                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# things we need for Tensorflow\n",
    "\n",
    "# create a data structure to hold user context\n",
    "context = {}\n",
    "\n",
    "ERROR_THRESHOLD = 0.25\n",
    "\n",
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern\n",
    "#     sentence_words = nltk.word_tokenize(sentence)\n",
    "    pos_result = twitter.pos(sentence, norm=True, stem=True)\n",
    "    print(\"sentence pos_result[%s]\" % pos_result)\n",
    "    print(\"sentence pos_result[0][0][%s]\" % pos_result[0][0])\n",
    "    print(\"sentence pos_result[1][0][%s]\" % pos_result[1][0])\n",
    "    print(\"sentence pos_result[][%s]\" % pos_result[0])\n",
    "    sentence_words = [lex for lex, pos in pos_result]\n",
    "    # stem each word\n",
    "#     sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def bow(sentence, words, show_details=False):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "\n",
    "    return(np.array(bag))\n",
    "\n",
    "\n",
    "def classify(sentence, words, classes, model):\n",
    "    # generate probabilities from the model\n",
    "    results = model.predict([bow(sentence, words)])[0]\n",
    "    # filter out predictions below a threshold\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
    "    # sort by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append((classes[r[0]], r[1]))\n",
    "    # return tuple of intent and probability\n",
    "    return return_list\n",
    "\n",
    "def response(sentence, intents, words, classes, model, userID='123', show_details=False):\n",
    "    results = classify(sentence, words, classes, model)\n",
    "    # if we have a classification then find the matching intent tag\n",
    "    if results:\n",
    "        # loop as long as there are matches to process\n",
    "        while results:\n",
    "            for i in intents['intents']:\n",
    "                # find a tag matching the first result\n",
    "                if i['tag'] == results[0][0]:\n",
    "                    # set context for this intent if necessary\n",
    "                    if 'context_set' in i:\n",
    "                        if show_details: print ('context:', i['context_set'])\n",
    "                        context[userID] = i['context_set']\n",
    "\n",
    "                    # check if this intent is contextual and applies to this user's conversation\n",
    "                    if not 'context_filter' in i or \\\n",
    "                        (userID in context and 'context_filter' in i and i['context_filter'] == context[userID]):\n",
    "                        if show_details: print ('tag:', i['tag'])\n",
    "                        # a random response from the intent\n",
    "                        return random.choice(i['responses'])\n",
    "\n",
    "            results.pop(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence pos_result[[('비젼', 'Noun'), ('은', 'Josa'), ('무엇', 'Noun'), ('입', 'Noun'), ('니까', 'Josa'), ('?', 'Punctuation')]]\n",
      "sentence pos_result[0][0][비젼]\n",
      "sentence pos_result[1][0][은]\n",
      "classify('비젼은 무엇입니까?')[[('vision', 0.9999989)]]\n",
      "sentence pos_result[[('비젼', 'Noun'), ('은', 'Josa'), ('무엇', 'Noun'), ('입', 'Noun'), ('니까', 'Josa'), ('?', 'Punctuation')]]\n",
      "sentence pos_result[0][0][비젼]\n",
      "sentence pos_result[1][0][은]\n",
      "response('비젼은 무엇입니까?') ==> [사람의 지적 상상력에 부합하고 창의력을 증진시키는 지능화된 지식 정보서비스 제공자가 되는 것입니다.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = '비젼은 무엇입니까?'\n",
    "print(\"classify('비젼은 무엇입니까?')[{}]\".format(classify(sentence, words, classes, model)))\n",
    "print(\"response('비젼은 무엇입니까?') ==> [{}]\".format(response(sentence, intents, words, classes, model)))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classify('무슨 제품이 있나요?')[[('product', 0.9999893)]]\n",
      "response('무슨 제품이 있나요?') ==> [채팅 대화하는 홈페이지, 채팅 대화 고객 FAQ, 채팅 대화 고객 문의 자동 응답 시스템을 클라우드 서비스와 패키지 기반 구축 서비스로 제공합니다.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = '무슨 제품이 있나요?'\n",
    "print(\"classify('무슨 제품이 있나요?')[%s]\" % classify(sentence, words, classes, model))\n",
    "print(\"response('무슨 제품이 있나요?') ==> [%s]\" % response(sentence, intents, words, classes, model))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'123': 'serviceTechnology'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show context\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classify('구체적인 기술은?')[[('possessTechnology', 0.99998426)]]\n",
      "response('구체적인 기술은?') ==> [부트스트랩 (CSS 프레임워크),jQuery(제이쿼리),머신러닝, 텐서플로우(TensorFlow),자연어처리(NLP),자연어이해(NLU),Python등입니다.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = '구체적인 기술은?'\n",
    "print(\"classify('구체적인 기술은?')[%s]\" % classify(sentence, words, classes, model))\n",
    "print(\"response('구체적인 기술은?') ==> [%s]\" % response(sentence, intents, words, classes, model))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'123': 'serviceTechnology'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show context\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classify('제품기술')[[('possessTechnology', 0.99270284)]]\n",
      "response('제품기술') ==> [부트스트랩 (CSS 프레임워크),jQuery(제이쿼리),머신러닝, 텐서플로우(TensorFlow),자연어처리(NLP),자연어이해(NLU),Python등입니다.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = '제품기술'\n",
    "print(\"classify('제품기술')[{}]\".format(classify(sentence, words, classes, model)))\n",
    "print(\"response('제품기술') ==> [{}]\".format(response(sentence, intents, words, classes, model)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: \n",
      "tag: greeting\n",
      "response('안녕?') ==> [만나서 반갑습니다]\n",
      "classify('안녕?')[[('greeting', 0.9996189)]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clear context\n",
    "#response(\"Hi there!\", show_details=True)\n",
    "sentence = '안녕'\n",
    "print(\"response('안녕?') ==> [{}]\".format(response(sentence, intents, words, classes, model,show_details=True)))\n",
    "print(\"classify('안녕?')[{}]\".format(classify(sentence, words, classes, model)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classify('서비스기술')[[('possessTechnology', 0.99912864)]]\n",
      "response('서비스기술') ==> [None]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = '서비스기술'\n",
    "print(\"classify('서비스기술')[{}]\".format(classify(sentence, words, classes, model)))\n",
    "print(\"response('서비스기술') ==> [{}]\".format(response(sentence, intents, words, classes, model)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'123': ''}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show context\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classify('안녕히 계셔요.')[[('goodbye', 0.99912757)]]\n",
      "response('안녕히 계셔요.') ==> [안녕히 가세요]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = '안녕히 계셔요.'\n",
    "print(\"classify('안녕히 계셔요.')[{}]\".format(classify(sentence, words, classes, model)))\n",
    "print(\"response('안녕히 계셔요.') ==> [{}]\".format(response(sentence, intents, words, classes, model)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'123': ''}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show context\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classify('당일 이요?')[[('contact', 0.960101)]]\n",
      "response('당일 이요?') ==> [문의 사항 있으시면 arkwith7@gmail.com으로 메일 주시면, 최선을 다해 답변 드리겠습니다.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = '뭐야?'\n",
    "print(\"classify('당일 이요?')[{}]\".format(classify(sentence, words, classes, model)))\n",
    "print(\"response('당일 이요?') ==> [{}]\".format(response(sentence, intents, words, classes, model)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
