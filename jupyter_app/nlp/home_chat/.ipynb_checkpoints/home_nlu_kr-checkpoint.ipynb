{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 9999  | total loss: \u001b[1m\u001b[32m0.62170\u001b[0m\u001b[0m | time: 0.041s\n",
      "| Adam | epoch: 1000 | loss: 0.62170 - acc: 0.9651 -- iter: 72/74\n",
      "Training Step: 10000  | total loss: \u001b[1m\u001b[32m0.55953\u001b[0m\u001b[0m | time: 0.044s\n",
      "| Adam | epoch: 1000 | loss: 0.55953 - acc: 0.9686 -- iter: 74/74\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\phs\\textmining\\python\\text-mining-camp\\note\\arkwith\\home_chat\\NLUModel\\model_home_kr.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n",
      "p is Bag of word for '오늘 가게  여나요?' :[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "classes :['Slang', 'contact', 'goodbye', 'greeting', 'history', 'keyCustomers', 'mission', 'possessTechnology', 'product', 'service', 'thanks', 'vision']\n",
      "model.predict([p]) :[[1.2514025e-03 5.7955305e-07 4.8065453e-04 9.8171186e-01 2.1619460e-09\n",
      "  3.8542622e-03 1.2965342e-12 1.2701326e-02 4.1987360e-12 1.7774669e-08\n",
      "  5.3227147e-08 3.2935158e-11]]\n",
      "74 documents\n",
      "12 classes ['Slang', 'contact', 'goodbye', 'greeting', 'history', 'keyCustomers', 'mission', 'possessTechnology', 'product', 'service', 'thanks', 'vision']\n",
      "108 unique stemmed words ['.', 'mission', 'vision', '가', '감사', '거기', '견적', '계세', '계시다', '고객', '고맙다', '과', '구입', '구체', '그렇다', '기술', '꺼지다', '나중', '날', '내용', '년', '누가', '누구', '는', '니까', '다음', '닥치다', '대하다', '도입', '돈', '되다', '되어다', '들리다', '또', '많이', '말', '멍청이', '몇', '목적', '무슨', '무엇', '문의', '뭐', '미션', '바보', '버세', '번', '보유', '비젼', '사라지다', '사업', '사용자', '새끼', '서비스', '설립', '수고', '수요', '술', '시장', '실적', '아키텍쳐', '안녕', '안녕하다', '야', '어떻다', '언제', '얼마나', '에', '에요', '여기', '연락', '연락처', '왜', '요', '위', '은', '을', '이', '이다', '이루다', '이에요', '인', '인가요', '임마', '입', '있다', '자다', '장사', '적', '적용', '제품', '좋다', '주', '주요', '줄다', '지내다', '창업', '채용', '처', '층', '친절하다', '타다', '판매', '하다', '해', '핵심', '활동', '회사']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Created on 2018. 2. 14.\n",
    "\n",
    "@author: phs\n",
    "'''\n",
    "\"\"\"\n",
    "1.대화 말뭉치 파일을 읽어들인다.\n",
    "2.대화 말뭉치를 읽어서 자연어 처리 및  Bag of word 생성\n",
    "3.Bag of word를 딥러닝 알고리즘 활용을 위한 입력으로 변환\n",
    "4.딥러닝(tensorflow)을 통한 자연어 이해 모델 생성\n",
    "5.자연어 이해 모델을 관리한다(저장,읽기)\n",
    "\"\"\"\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\"\n",
    "\n",
    "# things we need for NLP\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# import nltk\n",
    "# from nltk.stem.lancaster import LancasterStemmer\n",
    "# stemmer = LancasterStemmer()\n",
    "\n",
    "# from konlpy.tag import Komoran\n",
    "from konlpy.tag import Twitter\n",
    "# komoran = Komoran()\n",
    "twitter = Twitter()\n",
    "\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import numpy as np\n",
    "\n",
    "def read_dialog_intents_jsonfile(input_file_name):\n",
    "    \"\"\"\n",
    "     대화 말뭉치 파일을 읽어들인다.\n",
    "    \"\"\"\n",
    "    with open(input_file_name, 'rt', encoding='UTF8') as json_data:\n",
    "        intents = json.load(json_data)\n",
    "        \n",
    "    return intents\n",
    "    \n",
    "def dialog_nlp_processing(intents):\n",
    "    \"\"\"\n",
    "     대화 말뭉치를 읽어서 자연어 처리 및  Bag of word 생성\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    classes = []\n",
    "    documents = []\n",
    "    ignore_words = ['?']\n",
    "    # loop through each sentence in our intents patterns\n",
    "    for intent in intents['intents']:\n",
    "        for pattern in intent['patterns']:\n",
    "            # tokenize each word in the sentence\n",
    "#             w = nltk.word_tokenize(pattern)\n",
    "            pos_result = twitter.pos(pattern, norm=True, stem=True)\n",
    "            w = [lex for lex, pos in pos_result]\n",
    "            # add to our words list\n",
    "            words.extend(w)\n",
    "            # add to documents in our corpus\n",
    "            documents.append((w, intent['tag']))\n",
    "            # add to our classes list\n",
    "            if intent['tag'] not in classes:\n",
    "                classes.append(intent['tag'])\n",
    "    \n",
    "    # stem and lower each word and remove duplicates\n",
    "#     words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "#     words = sorted(list(set(words)))\n",
    "    words = [w for w in words if w not in ignore_words]\n",
    "    words = sorted(list(set(words)))\n",
    "    \n",
    "    # remove duplicates\n",
    "    classes = sorted(list(set(classes)))\n",
    "    \n",
    "    return classes, documents, words\n",
    "\n",
    "def prepare_machine_learning(classes, documents, words):\n",
    "    \"\"\"\n",
    "    Bag of word를 딥러닝 알고리즘 활용을 위한 입력으로 변환\n",
    "    \"\"\"\n",
    "    \n",
    "    # create our training data\n",
    "    training = []\n",
    "    output_row = []\n",
    "    # create an empty array for our output\n",
    "    output_empty = [0] * len(classes)\n",
    "    \n",
    "    # training set, bag of words for each sentence\n",
    "    for doc in documents:\n",
    "        # initialize our bag of words\n",
    "        bag = []\n",
    "        # list of tokenized words for the pattern\n",
    "        pattern_words = doc[0]\n",
    "        # stem each word\n",
    "#         pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "        # create our bag of words array\n",
    "        for w in words:\n",
    "            bag.append(1) if w in pattern_words else bag.append(0)\n",
    "    \n",
    "        # output is a '0' for each tag and '1' for current tag\n",
    "        output_row = list(output_empty)\n",
    "        output_row[classes.index(doc[1])] = 1\n",
    "    \n",
    "        training.append([bag, output_row])\n",
    "    \n",
    "    # shuffle our features and turn into np.array\n",
    "    random.shuffle(training)\n",
    "    training = np.array(training)\n",
    "    \n",
    "    # create train and test lists\n",
    "    train_x = list(training[:,0])\n",
    "    train_y = list(training[:,1])\n",
    "\n",
    "    return train_x, train_y\n",
    "\n",
    "def create_tensorflow_learning_model(train_x, train_y, output_model_file_name):\n",
    "    \"\"\"\n",
    "    딥러닝(tensorflow)을 통한 자연어 이해 모델 생성\n",
    "    \"\"\"\n",
    "    \n",
    "    # reset underlying graph data\n",
    "    tf.reset_default_graph()\n",
    "    # Build neural network\n",
    "    net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "    net = tflearn.fully_connected(net, 8)\n",
    "    net = tflearn.fully_connected(net, 8)\n",
    "    net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
    "    net = tflearn.regression(net)\n",
    "    \n",
    "    # Define model and setup tensorboard\n",
    "    model = tflearn.DNN(net, tensorboard_dir='home_tflearn_kr_logs')\n",
    "    # Start training (apply gradient descent algorithm)\n",
    "    model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "    # save the trained model to directory\n",
    "    model.save(output_model_file_name)\n",
    "\n",
    "    return model\n",
    "\n",
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern\n",
    "#     sentence_words = nltk.word_tokenize(sentence)\n",
    "    pos_result = twitter.pos(sentence, norm=True, stem=True)\n",
    "    sentence_words = [lex for lex, pos in pos_result]\n",
    "    # stem each word\n",
    "#     sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def bow(sentence, words, show_details=False):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "\n",
    "    return(np.array(bag))\n",
    "\n",
    "# save all of our data structures\n",
    "def save_training_data_structures(words, classes, train_x, train_y, output_training_data_file_name):\n",
    "    \"\"\"\n",
    "    자연어 이해 모델을 관리한다(저장,읽기)\n",
    "    \"\"\"\n",
    "    # save all of our data structures\n",
    "    pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( output_training_data_file_name, \"wb\" ) )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # 대화 말뭉치 파일을 읽어들인다.\n",
    "    input_file_name = './DialogIntents/intents_home_kr.json'\n",
    "    intents = read_dialog_intents_jsonfile(input_file_name)\n",
    "    \n",
    "    # 대화 말뭉치를 읽어서 자연어 처리 및  Bag of word 생성\n",
    "    classes, documents, words = dialog_nlp_processing(intents)\n",
    "    print (len(documents), \"documents\")\n",
    "    print (len(classes), \"classes\", classes)\n",
    "    print (len(words), \"unique stemmed words\", words)\n",
    "    \n",
    "    # Bag of word를 딥러닝 알고리즘 활용을 위한 입력으로 변환\n",
    "    train_x, train_y = prepare_machine_learning(classes, documents, words)\n",
    "    \n",
    "    # 딥러닝(tensorflow)을 통한 자연어 이해 모델 생성\n",
    "    output_model_file_name = './NLUModel/model_home_kr.tflearn'\n",
    "    model = create_tensorflow_learning_model(train_x, train_y, output_model_file_name)\n",
    "\n",
    "    # 자연어 이해 모델을 관리한다(저장,읽기)\n",
    "    output_training_data_file_name = \"./NLUModel/training_data_home_kr\"\n",
    "    save_training_data_structures(words, classes, train_x, train_y, output_training_data_file_name)\n",
    "    \n",
    "    p = bow(\"비젼은 무엇입니까?\", words)\n",
    "    print(\"p is Bag of word for '비젼은 무엇입니까?' :{}\".format(p))\n",
    "    print(\"classes :{}\".format(classes))\n",
    "    print(\"model.predict([p]) :{}\".format(model.predict([p])))\n",
    "    \n",
    "    print (len(documents), \"documents\")\n",
    "    print (len(classes), \"classes\", classes)\n",
    "    print (len(words), \"unique stemmed words\", words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
