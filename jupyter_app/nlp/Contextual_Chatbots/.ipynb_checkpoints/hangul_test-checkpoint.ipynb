{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 documents\n",
      "1 classes ['여행']\n",
      "25 unique stemmed words ['.', '2', '5', 'ㅂ니다', '가', '감사', '기차표', '까지', '는데요', '대전', '또', '만', '수고', '시', '안녕하세요', '어디', '어요', '얼마', '에', '오', '요', '원', '이', '천', '하']\n"
     ]
    }
   ],
   "source": [
    "# things we need for NLP\n",
    "# import nltk\n",
    "# from nltk.stem.lancaster import LancasterStemmer\n",
    "# stemmer = LancasterStemmer()\n",
    "\n",
    "from konlpy.tag import Komoran\n",
    "from konlpy.tag import Twitter\n",
    "komoran = Komoran()\n",
    "twitter = Twitter()\n",
    "\n",
    "sentences = [\"안녕하세요\", \"기차표 얼마에요?\",\"어디까지 가시는데요?\",\"대전입니다.\",\"2만5천원입니다.\",\"또 오세요\",\"수고하세요\",\"감사합니다.\"]\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?']\n",
    "# loop through each sentence in our intents patterns\n",
    "for sentence in sentences:\n",
    "    # tokenize each word in the sentence\n",
    "    w = komoran.morphs(sentence)\n",
    "    # add to our words list\n",
    "    words.extend(w)\n",
    "    # add to documents in our corpus\n",
    "    documents.append((w, \"여행\"))\n",
    "    # add to our classes list\n",
    "    classes.append(\"여행\")\n",
    "\n",
    "# stem and lower each word and remove duplicates\n",
    "words = [w for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "# remove duplicates\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print (len(documents), \"documents\")\n",
    "print (len(classes), \"classes\", classes)\n",
    "print (len(words), \"unique stemmed words\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['원칙', '이나', '기체', '설계', '와', '엔진', '·', '레이더', '·', '항법', '장비', '등', '이것', '도', '되나욬ㅋㅋ']\n",
      "['원칙', '기체', '설계', '엔진', '레이더', '항법', '장비', '등']\n",
      "[('원칙', 'NNG'), ('이나', 'JC'), ('기체', 'NNG'), ('설계', 'NNG'), ('와', 'JC'), ('엔진', 'NNG'), ('·', 'SP'), ('레이더', 'NNG'), ('·', 'SP'), ('항법', 'NNP'), ('장비', 'NNG'), ('등', 'NNB'), ('이것', 'NP'), ('도', 'JX'), ('되나욬ㅋㅋ', 'NA')]\n"
     ]
    }
   ],
   "source": [
    "# from konlpy.tag import Komoran\n",
    "# komoran = Komoran()\n",
    "print(komoran.morphs('원칙이나 기체 설계와 엔진·레이더·항법장비 등 이것도 되나욬ㅋㅋ'))\n",
    "print(komoran.nouns('원칙이나 기체 설계와 엔진·레이더·항법장비 등 이것도 되나욬ㅋㅋ!'))\n",
    "print(komoran.pos('원칙이나 기체 설계와 엔진·레이더·항법장비 등 이것도 되나욬ㅋㅋ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['원칙', '이나', '기체', '설계', '와', '엔진', '·', '레이더', '·', '항법', '장비', '등', '이', '것', '도', '되나욬', 'ㅋㅋ']\n",
      "['원칙', '기체', '설계', '엔진', '레이더', '항법', '장비', '등', '것', '되나욬']\n",
      "['원칙', '기체 설계', '기체 설계와 엔진', '설계와 엔진', '엔진', '레이더', '항법장비 등 이것', '등 이것', '기체', '설계', '항법', '장비', '이것']\n",
      "[('원칙', 'Noun'), ('이나', 'Josa'), ('기체', 'Noun'), ('설계', 'Noun'), ('와', 'Josa'), ('엔진', 'Noun'), ('·', 'Foreign'), ('레이더', 'Noun'), ('·', 'Foreign'), ('항법', 'Noun'), ('장비', 'Noun'), ('등', 'Noun'), ('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되나욬', 'Noun'), ('ㅋㅋ', 'KoreanParticle')]\n",
      "[('원칙', 'Noun'), ('이나', 'Josa'), ('기체', 'Noun'), ('설계', 'Noun'), ('와', 'Josa'), ('엔진', 'Noun'), ('·', 'Foreign'), ('레이더', 'Noun'), ('·', 'Foreign'), ('항법', 'Noun'), ('장비', 'Noun'), ('등', 'Noun'), ('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되', 'Verb'), ('나요', 'Eomi'), ('ㅋㅋ', 'KoreanParticle')]\n",
      "[('원칙', 'Noun'), ('이나', 'Josa'), ('기체', 'Noun'), ('설계', 'Noun'), ('와', 'Josa'), ('엔진', 'Noun'), ('·', 'Foreign'), ('레이더', 'Noun'), ('·', 'Foreign'), ('항법', 'Noun'), ('장비', 'Noun'), ('등', 'Noun'), ('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되다', 'Verb'), ('ㅋㅋ', 'KoreanParticle')]\n"
     ]
    }
   ],
   "source": [
    "print(twitter.morphs('원칙이나 기체 설계와 엔진·레이더·항법장비 등 이것도 되나욬ㅋㅋ'))\n",
    "print(twitter.nouns('원칙이나 기체 설계와 엔진·레이더·항법장비 등 이것도 되나욬ㅋㅋ'))\n",
    "print(twitter.phrases('원칙이나 기체 설계와 엔진·레이더·항법장비 등 이것도 되나욬ㅋㅋ'))\n",
    "print(twitter.pos('원칙이나 기체 설계와 엔진·레이더·항법장비 등 이것도 되나욬ㅋㅋ'))\n",
    "print(twitter.pos('원칙이나 기체 설계와 엔진·레이더·항법장비 등 이것도 되나욬ㅋㅋ', norm=True))\n",
    "print(twitter.pos('원칙이나 기체 설계와 엔진·레이더·항법장비 등 이것도 되나욬ㅋㅋ', norm=True, stem=True))\n",
    "sentences = [\"안녕하세요\", \"기차표 얼마에요?\",\"어디까지 가시는데요?\",\"대전입니다.\",\"2만5천원입니다.\",\"또 오세요\",\"수고하세요\",\"감사합니다.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하다', '기차표', '얼마', '에요', '어디', '까지', '갈다', '대전', '이다', '2', '만', '5', '천원', '이다', '또', '오다', '수고', '하다', '감사', '하다']\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"안녕하세요\", \"기차표 얼마에요?\",\"어디까지 가시는데요?\",\"대전입니다.\",\"2만5천원입니다.\",\"또 오세요\",\"수고하세요\",\"감사합니다.\"]\n",
    "words = []\n",
    "ignore_words = ['?','.']\n",
    "for sentence in sentences:\n",
    "    # tokenize each word in the sentence\n",
    "    pos_result = twitter.pos(sentence, norm=True, stem=True)\n",
    "    w = [lex for lex, pos in pos_result]\n",
    "    words.extend(w)\n",
    "\n",
    "words = [w for w in words if w not in ignore_words]\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
