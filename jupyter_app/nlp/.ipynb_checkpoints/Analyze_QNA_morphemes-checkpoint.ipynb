{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 크롤링한 소요리문답자료를 형태소분석한다.\n",
    "\n",
    "### 크롤링한 소요리문답 JSON파일을 읽어서 문장단위로 나누어 형태소 분석을 한다.\n",
    "1. 소요리문답 JSON파일을 읽는다.\n",
    "1. 질문과 답변을 문장단위로 나누어 형태소 분석을 한다.\n",
    "1. 형태소 분석 결과를 후처리 한다.\n",
    "1. 형태소 분석 결과를 파일로 저장한다.\n",
    "    \n",
    "### 형태소 분석 결과를 다시 JSON파일로 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "from konlpy.tag import Hannanum\n",
    "import itertools\n",
    "import mmap\n",
    "from tqdm import tqdm_notebook\n",
    "import ujson\n",
    "import re\n",
    "\n",
    "def split_sentences(text):\n",
    "    \"\"\"주어진 텍스트를 문장 단위로 분절하여 돌려준다.\"\"\"\n",
    "    \n",
    "    all_sentences = []\n",
    "    lines = [line for line in text.strip().splitlines() if line.strip]\n",
    "    \n",
    "    for line in lines:\n",
    "        sentences = re.split(\"(?<=[.?!]) \", line)\n",
    "        all_sentences += sentences\n",
    "    \n",
    "    return all_sentences\n",
    "\n",
    "\n",
    "def get_morph_anal(analyzer, text):\n",
    "    \"\"\"주어진 형태소 분석기 객체로 텍스트를 문장 단위로 형태소 분석하여 돌려준다.\"\"\"\n",
    "    \n",
    "    sent_morph_anals = []\n",
    "    sentences = split_sentences(text)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sent_morph_anal = analyzer.pos(sentence)\n",
    "        sent_morph_anals.append(sent_morph_anal)\n",
    "        \n",
    "    return sent_morph_anals\n",
    "\n",
    "def find_sublists(seq, sublist):\n",
    "    length = len(sublist)\n",
    "    for index, value in enumerate(seq):\n",
    "        if value == sublist[0] and seq[index:index+length] == sublist:\n",
    "            yield index, index+length\n",
    "            \n",
    "\n",
    "def replace_sublist(seq, target, replacement, maxreplace=None):\n",
    "    sublists = find_sublists(seq, target)\n",
    "    if maxreplace:\n",
    "        sublists = itertools.islice(sublists, maxreplace)\n",
    "    for start, end in sublists:\n",
    "        seq[start:end] = replacement\n",
    "\n",
    "\n",
    "def get_num_lines(file_path):\n",
    "    \"\"\"빠른 속도로 텍스트 파일의 줄 수를 세어 돌려준다.\n",
    "    https://blog.nelsonliu.me/2016/07/29/progress-bars-for-python-file-reading-with-tqdm/\n",
    "    \"\"\"\n",
    "    \n",
    "    fp = open(file_path, \"r+\")\n",
    "    buf = mmap.mmap(fp.fileno(), 0)\n",
    "    lines = 0\n",
    "    while buf.readline():\n",
    "        lines += 1\n",
    "    return lines\n",
    "\n",
    "def refine_answer(answer):\n",
    "    \"\"\"소요리문답의 성경구절이 있는 부분을 형태소분석을 위해 제거\"\"\"\n",
    "    \n",
    "    #print(\"소스:{}\".format(answer))\n",
    "    find_str = \"(\" \n",
    "    str_index = answer.find(find_str)\n",
    "    if str_index > -1:\n",
    "        answer_desc = answer[:str_index]\n",
    "    else:\n",
    "        answer_desc = answer\n",
    "        \n",
    "    refined_answer = answer_desc.translate({ ord('「'): '',ord('」'): ' ' })\n",
    "        \n",
    "    return refined_answer\n",
    "    \n",
    "def parse_row(row):\n",
    "    \"\"\"주어진 행을 열 단위로 분절하여 돌려준다.\"\"\"\n",
    "\n",
    "    qna_no = row[\"no\"]\n",
    "    question_source = row[\"question\"]\n",
    "    answer_source = row[\"answer\"]\n",
    "    \n",
    "    return qna_no, question_source, answer_source\n",
    "\n",
    "\n",
    "def compose_json_doc(qna_no, question_source, answer_source, refine_answer, question_ma, answer_ma):\n",
    "    \"\"\"주어진 문서 요소들로 JSON 문서를 생성하여 돌려준다.\"\"\"\n",
    "    \n",
    "    json_doc = {\n",
    "        \"no\": int(qna_no), \n",
    "        \"question\": question_source, \n",
    "        \"answer\": answer_source, \n",
    "        \"refine_answer\": refine_answer, \n",
    "        \"question_ma\": question_ma, \n",
    "        \"answer_ma\": answer_ma              \n",
    "    }   \n",
    "        \n",
    "    return json_doc\n",
    "\n",
    "\n",
    "def write_json_doc(output_file, json_doc):\n",
    "    \"\"\"주어진 JSON 문서를 출력 파일에 기록한다.\"\"\"\n",
    "        \n",
    "    json_str = ujson.dumps(json_doc, ensure_ascii=False)\n",
    "    print(json_str, file=output_file)\n",
    "\n",
    "#=============================================================#\n",
    "# 부분 리스트 치환에 의한 형태소 분석 결과 후처리\n",
    "post_proc_pairs = [\n",
    "    ([('신', 'XPN'), ('구약 성경', 'NNP')], [('신구약', 'NNG'), ('성경', 'NNG')]),\n",
    "    ([('제', 'XPN'), ('일', 'NNB'), ('계명', 'NNG')], [('제 일 계명', 'NNG')]),\n",
    "    ([('제', 'XPN'), ('이', 'MM'), ('계명', 'NNG')], [('제 이 계명', 'NNG')]),\n",
    "    ([('제', 'XPN'), ('삼', 'NR'), ('계명', 'NNG')], [('제 삼 계명', 'NNG')]),\n",
    "    ([('제', 'XPN'), ('사', 'NNG'), ('계명', 'NNG')], [('제 사 계명', 'NNG')]),\n",
    "    ([('제', 'XPN'), ('오', 'NR'), ('계명', 'NNG')], [('제 오 계명', 'NNG')]),\n",
    "    ([('제', 'XPN'), ('육', 'NR'), ('계명', 'NNG')], [('제 육 계명', 'NNG')]),\n",
    "    ([('제', 'XPN'), ('치', 'VV'), ('ㄹ', 'ETM'), ('계명', 'NNG')], [('제 칠 계명', 'NNG')]),\n",
    "    ([('제', 'XPN'), ('팔', 'VV'), ('ㄹ', 'ETM'), ('계명', 'NNG')], [('제 팔 계명', 'NNG')]),\n",
    "    ([('제', 'XPN'), ('구', 'NNG'), ('계명', 'NNG')], [('제 구 계명', 'NNG')]),\n",
    "    ([('제', 'XPN'), ('십', 'NR'), ('계명', 'NNG')], [('제 십 계명', 'NNG')]),\n",
    "    ([('첫','MM'), ('말씀','NNG')], [('첫','NR'),('말씀','NNG')]),\n",
    "    ([('첫','MM'), ('기도','NNG')], [('첫','NR'),('기도','NNG')])\n",
    "]\n",
    "\n",
    "\n",
    "input_file_name = \"data/crawling/catechism.txt\"\n",
    "output_file_name = \"data/catechism/catechism.ma.txt\"\n",
    "komoran = Komoran()\n",
    "hannanum = Hannanum()\n",
    "\n",
    "\n",
    "with open(input_file_name, \"r\", encoding=\"utf-8\") as input_file, \\\n",
    "        open(output_file_name, \"w\", encoding=\"utf-8\") as output_file:\n",
    "        \n",
    "    for line in tqdm_notebook(input_file, desc=\"Reading documents\", \n",
    "                                  total=get_num_lines(input_file_name)):\n",
    "        \n",
    "        doc_row = ujson.loads(line)\n",
    "        qna_no, question_source, answer_source = parse_row(doc_row)\n",
    "        refined_answer = refine_answer(answer_source)        \n",
    "\n",
    "        #print(\"질문[{}]:{}\".format(qna_no, question_source.translate({ ord('「'): '',ord('」'): ' ' })))\n",
    "        #print(\"답변:{}\".format(refined_answer))\n",
    "        \n",
    "        question_ma = get_morph_anal(komoran, question_source.translate({ ord('「'): '',ord('」'): ' ' }) )\n",
    "        # 소요리 문답 질문에 대한 부분 리스트 치환에 의한 형태소 분석 결과 후처리\n",
    "        for src, dst in post_proc_pairs:\n",
    "            replace_sublist(question_ma[0], src, dst)\n",
    "\n",
    "        answer_ma = get_morph_anal(komoran, refined_answer)\n",
    "        # 소요리 문답 답변에 대한 부분 리스트 치환에 의한 형태소 분석 결과 후처리\n",
    "        for src, dst in post_proc_pairs:\n",
    "            replace_sublist(answer_ma[0], src, dst)\n",
    "\n",
    "        #print(\"질문[{}]:{}\".format(qna_no, question_ma))\n",
    "        #print(\"답변:{}\".format(answer_ma))\n",
    "        #print(\"{}\".format(\"=\"*80))\n",
    "        json_doc = compose_json_doc(qna_no, question_source, answer_source, refined_answer, question_ma, answer_ma)\n",
    "        write_json_doc(output_file, json_doc)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
