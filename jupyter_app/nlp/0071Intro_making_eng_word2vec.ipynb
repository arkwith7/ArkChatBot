{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e150264",
   "metadata": {},
   "source": [
    "## 1. 영어 Word2Vec 만들기\n",
    "파이썬의 gensim 패키지에는 Word2Vec을 지원하고 있어, gensim 패키지를 이용하면 손쉽게 단어를 임베딩 벡터로 변환시킬 수 있습니다. 영어로 된 코퍼스를 다운받아 전처리를 수행하고, 전처리한 데이터를 바탕으로 Word2Vec 작업을 진행하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84f47fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from lxml import etree\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce073b00",
   "metadata": {},
   "source": [
    "### 1) 훈련 데이터 이해하기\n",
    "훈련 데이터를 다운로드 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b17cbf56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/word2vec_data/ted_en-20160408.xml',\n",
       " <http.client.HTTPMessage at 0x7f5a442d3730>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 다운로드\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/ted_en-20160408.xml\", filename=\"data/word2vec_data/ted_en-20160408.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837dd88d",
   "metadata": {},
   "source": [
    "훈련 데이터 파일은 xml 문법으로 작성되어 있어 자연어를 얻기 위해서는 전처리가 필요합니다. 얻고자 하는 실질적 데이터는 영어문장으로만 구성된 내용을 담고 있는 <content>와 </content> 사이의 내용입니다. 전처리 작업을 통해 xml 문법들은 제거하고, 해당 데이터만 가져와야 합니다. 뿐만 아니라, <content>와 </content> 사이의 내용 중에는 (Laughter)나 (Applause)와 같은 배경음을 나타내는 단어도 등장하는데 이 또한 제거해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d0dd1e",
   "metadata": {},
   "source": [
    "```\n",
    "<file id=\"1\">\n",
    "  <head>\n",
    "<url>http://www.ted.com/talks/knut_haanaes_two_reasons_companies_fail_and_how_to_avoid_them</url>\n",
    "       <pagesize>72832</pagesize>\n",
    "... xml 문법 중략 ...\n",
    "<content>\n",
    "Here are two reasons companies fail: they only do more of the same, or they only do what's new.\n",
    "To me the real, real solution to quality growth is figuring out the balance between two activities:\n",
    "... content 내용 중략 ...\n",
    "To me, the irony about the Facit story is hearing about the Facit engineers, who had bought cheap, small electronic calculators in Japan that they used to double-check their calculators.\n",
    "(Laughter)\n",
    "... content 내용 중략 ...\n",
    "(Applause)\n",
    "</content>\n",
    "</file>\n",
    "<file id=\"2\">\n",
    "    <head>\n",
    "<url>http://www.ted.com/talks/lisa_nip_how_humans_could_evolve_to_survive_in_space<url>\n",
    "... 이하 중략 ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da3e12d",
   "metadata": {},
   "source": [
    "### 2) 훈련 데이터 전처리하기\n",
    "위 데이터를 위한 전처리 코드는 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64d3f6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "targetXML = open('data/word2vec_data/ted_en-20160408.xml', 'r', encoding='UTF8')\n",
    "target_text = etree.parse(targetXML)\n",
    "\n",
    "# xml 파일로부터 <content>와 </content> 사이의 내용만 가져온다.\n",
    "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
    "\n",
    "# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (Audio), (Laughter) 등의 배경음 부분을 제거.\n",
    "# 해당 코드는 괄호로 구성된 내용을 제거.\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
    "\n",
    "# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화를 수행.\n",
    "sent_text = sent_tokenize(content_text)\n",
    "\n",
    "# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\n",
    "normalized_text = []\n",
    "for string in sent_text:\n",
    "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
    "     normalized_text.append(tokens)\n",
    "\n",
    "# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행.\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e498633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플의 개수 : 273380\n"
     ]
    }
   ],
   "source": [
    "print('총 샘플의 개수 : {}'.format(len(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daf2c0f",
   "metadata": {},
   "source": [
    "총 샘플의 개수는 약 27만 3천개입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62d4249b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n",
      "['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']\n",
      "['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']\n"
     ]
    }
   ],
   "source": [
    "# 샘플 3개만 출력\n",
    "for line in result[:3]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edec8d6f",
   "metadata": {},
   "source": [
    "상위 3개 문장만 출력해보았는데 토큰화가 수행되었음을 볼 수 있습니다. Word2Vec 모델에 텍스트 데이터를 훈련시킵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426fb472",
   "metadata": {},
   "source": [
    "### 3) Word2Vec 훈련시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a09a2d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# model = Word2Vec(sentences=result, size=100, window=5, min_count=5, workers=4, sg=0)\n",
    "model = Word2Vec(sentences=result, vector_size=100, window=5, min_count=5, workers=4, sg=0)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17b25bd",
   "metadata": {},
   "source": [
    "Word2Vec의 하이퍼파라미터값은 다음과 같습니다.\n",
    "\n",
    "* size = 워드 벡터의 특징 값. 즉, 임베딩 된 벡터의 차원.   \n",
    "* window = 컨텍스트 윈도우 크기  \n",
    "* min_count = 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 학습하지 않는다.)  \n",
    "* workers = 학습을 위한 프로세스 수  \n",
    "* sg = 0은 CBOW, 1은 Skip-gram.  \n",
    "\n",
    "Word2Vec에 대해서 학습을 진행하였습니다. Word2Vec는 입력한 단어에 대해서 가장 유사한 단어들을 출력하는 model.wv.most_similar을 지원합니다. man과 가장 유사한 단어들은 어떤 단어들일까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ee8d67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.8446374535560608), ('guy', 0.8234966993331909), ('lady', 0.7744773626327515), ('boy', 0.7652905583381653), ('girl', 0.7357457280158997), ('gentleman', 0.7313880324363708), ('kid', 0.7204859852790833), ('soldier', 0.6971893310546875), ('poet', 0.6797811985015869), ('friend', 0.6608587503433228)]\n"
     ]
    }
   ],
   "source": [
    "model_result = model.wv.most_similar(\"man\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f131ff",
   "metadata": {},
   "source": [
    "man과 유사한 단어로 woman, guy, boy, lady, girl, gentleman, soldier, kid 등을 출력하는 것을 볼 수 있습니다. Word2Vec를 통해 단어의 유사도를 계산할 수 있게 되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6783c99",
   "metadata": {},
   "source": [
    "### 4) Word2Vec 모델 저장하고 로드하기\n",
    "공들여 학습한 모델을 언제든 나중에 다시 사용할 수 있도록 컴퓨터 파일로 저장하고 다시 로드해보겠습니다. 이 모델을 가지고 향후 시각화를 진행할 예정이므로 꼭 저장해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dca30e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('model_repo/eng_w2v') # 모델 저장\n",
    "loaded_model = KeyedVectors.load_word2vec_format(\"model_repo/eng_w2v\") # 모델 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24949c90",
   "metadata": {},
   "source": [
    "로드한 모델에 대해서 다시 man과 유사한 단어를 출력해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4ff4699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.8446374535560608), ('guy', 0.8234966993331909), ('lady', 0.7744773626327515), ('boy', 0.7652905583381653), ('girl', 0.7357457280158997), ('gentleman', 0.7313880324363708), ('kid', 0.7204859852790833), ('soldier', 0.6971893310546875), ('poet', 0.6797811985015869), ('friend', 0.6608587503433228)]\n"
     ]
    }
   ],
   "source": [
    "model_result = loaded_model.most_similar(\"man\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b033a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
